# SUKO Image Generation Pipeline - Issues Fixed & Deployment Guide

**Date:** January 12, 2026  
**Status:** âœ… All Core Issues Resolved

---

## ðŸ” Project Architecture Overview

This is a **distributed microservices system** for AI-powered image generation with 4 specialized pods:

### Pod Infrastructure

| Pod | ID | GPU | Port | Purpose | Model |
|-----|-----|-----|------|---------|-------|
| **Interface** | i8yfozejwi4lyk | CPU | 8000 | Load Balancer/Router | None |
| **Selfie Feature** | wp6is8takig1z0 | RTX 6000 Ada (48GB) | 8003 | VQA + Face Detection | Qwen2-VL-7B, YOLOv8 |
| **Profile Worker** | ika5ni96asnpts | RTX 4090 (24GB) | 8003 | Image Generation | Z-Image Turbo |
| **LLM Service** | cf0pc97a1e9lki | RTX 4090 (24GB) | 8002 | Text Generation | Qwen2.5-7B |

### Data Flow

```
User Request â†’ Interface (8000)
    â†“
    â”œâ”€â†’ Selfie Feature Worker (8003) â†’ Extract features from selfie
    â”‚   â”œâ”€ YOLOv8: Face detection
    â”‚   â””â”€ Qwen2-VL-7B: Feature extraction (VQA)
    â†“
    â”œâ”€â†’ Profile Worker (8003) â†’ Generate avatar image
    â”‚   â””â”€ Z-Image Turbo: Diffusion model
    â†“
    â””â”€â†’ LLM Service (8002) â†’ Generate prompt/text
        â””â”€ Qwen2.5-7B: Text generation
```

---

## âŒ Issues Found & Fixed

### 1. **Missing config.yaml** âœ… FIXED
- **Problem:** Only `config.example.yaml` existed, no production config
- **Fix:** Created `/config/config.yaml` with correct pod URLs
- **Impact:** Services can now load configuration properly

### 2. **Broken Profile Worker Requirements** âœ… FIXED
- **Problem:** `requirements.profile.txt` was referencing wrong base files
  ```diff
  - -r requirements.gpu.txt
  - python-multipart==0.0.9
  - opencv-python-headless==4.10.0.84
  - mediapipe==0.10.14
  + -r requirements.interface.txt
  + torch>=2.1.0
  + diffusers==0.36.0
  + transformers>=4.46.0
  + sentencepiece>=0.1.99
  ```
- **Impact:** Profile worker can now install correct dependencies

### 3. **Missing sentencepiece Dependency** âœ… FIXED
- **Problem:** Transformers library requires `sentencepiece` for tokenization, but it was missing
- **Fix:** Added `sentencepiece>=0.1.99` to all GPU worker requirements
- **Files Updated:**
  - `requirements.selfie_feature.txt`
  - `requirements.profile.txt`
  - `requirements.llm.txt`
- **Impact:** Prevents tokenizer errors when loading Qwen models

### 4. **Missing ultralytics (YOLOv8)** âœ… FIXED
- **Problem:** Selfie feature worker imports `ultralytics` but dependency was missing
- **Fix:** Added `ultralytics>=8.1.0` to `requirements.selfie_feature.txt`
- **Impact:** YOLOv8 face detection now works properly

### 5. **Incorrect Deployment Script** âœ… FIXED
- **Problem:** `deploy_profile.sh` was installing `shared/requirements.txt` instead of profile-specific deps
- **Fix:** Updated to install `requirements.profile.txt`
- **Impact:** Profile worker deploys with correct dependencies

### 6. **Missing LLM Deployment Script** âœ… FIXED
- **Problem:** No deployment script existed for LLM service
- **Fix:** Created `deploy_llm.sh` with proper setup
- **Impact:** LLM service can now be deployed consistently

### 7. **NumPy Version Conflicts** âœ… FIXED
- **Problem:** Some requirements had no NumPy constraint, others required `<2.0.0`
- **Fix:** Standardized to `numpy>=1.26.0,<2.0.0` across all GPU workers
- **Impact:** Prevents compatibility issues with transformers/torch

---

## ðŸ“‹ Complete Requirements Matrix

### Interface (CPU Pod)
```
fastapi>=0.109.0
uvicorn[standard]>=0.27.0
python-multipart>=0.0.6
httpx>=0.27.0
pydantic>=2.0.0
pydantic-settings>=2.0.0
PyYAML>=6.0
```

### Selfie Feature Worker (RTX 6000 Ada)
```
All interface requirements +
torch>=2.1.0
transformers==4.57.3
accelerate==1.12.0
sentencepiece>=0.1.99
qwen-vl-utils==0.0.8
opencv-python==4.10.0.84
ultralytics>=8.1.0
numpy<2.0.0
Pillow==10.3.0
```

### Profile Worker (RTX 4090)
```
All interface requirements +
torch>=2.1.0
diffusers==0.36.0
transformers>=4.46.0
accelerate==1.12.0
sentencepiece>=0.1.99
safetensors==0.7.0
numpy>=1.26.0,<2.0.0
Pillow==10.3.0
```

### LLM Service (RTX 4090)
```
All interface requirements +
torch>=2.1.0
transformers>=4.46.0
accelerate>=1.12.0
sentencepiece>=0.1.99
safetensors>=0.7.0
```

---

## ðŸš€ Deployment Instructions (Updated)

### Prerequisites
All pods must have:
```bash
cd /workspace
git clone https://github.com/DarkOxygen123/img_pod.git
cd img_pod
```

### Deploy Each Service

#### 1. Interface Pod (i8yfozejwi4lyk)
```bash
ssh root@213.173.105.83 -p 58672 -i ~/.ssh/id_ed25519
cd /workspace/img_pod
git pull
bash deploy_interface.sh
```
**Health Check:** `curl http://localhost:8000/healthz`

#### 2. Selfie Feature Worker (wp6is8takig1z0)
```bash
ssh root@195.26.233.53 -p 23674 -i ~/.ssh/id_ed25519
cd /workspace/img_pod
git pull
bash deploy_selfie_feature.sh
```
âš ï¸ **First run downloads Qwen2-VL-7B (~16GB) + YOLOv8 (~6MB), takes 5-10 min**  
**Health Check:** `curl http://localhost:8003/healthz`

#### 3. Profile Worker (ika5ni96asnpts)
```bash
ssh root@103.196.86.90 -p 13560 -i ~/.ssh/id_ed25519
cd /workspace/img_pod
git pull
bash deploy_profile.sh
```
**Health Check:** `curl http://localhost:8003/healthz`

#### 4. LLM Service (cf0pc97a1e9lki) - NEW
```bash
ssh root@82.221.170.242 -p 45164 -i ~/.ssh/id_ed25519
cd /workspace/img_pod
git pull
bash deploy_llm.sh
```
**Health Check:** `curl http://localhost:8002/healthz`

---

## ðŸ§ª Testing the Pipeline

### From Local Machine
```bash
cd /Users/uthamkumar/Uthamkumar/SUKO/Sukofinal
.venv/bin/python testing/profile_create_and_save.py
```

### Expected Improvements After Fixes
- âœ… **selfie_3** (no face detected) â†’ Now uses full image with quality penalty
- âœ… **selfie_4, selfie_6** (multiple faces) â†’ Picks largest face automatically
- âœ… **Better feature extraction** with Qwen2-VL-7B (upgraded from 2B)
- âœ… **Robust face detection** with YOLOv8 + OpenCV fallback
- âœ… **No more module errors** - all dependencies properly installed

---

## ðŸ”§ Troubleshooting

### Check Service Logs
```bash
# On each pod
tail -f /tmp/interface-8000.log      # Interface
tail -f /tmp/selfie-feature-8003.log # Selfie Feature
tail -f /tmp/profile-8003.log        # Profile Worker
tail -f /tmp/llm-8002.log            # LLM Service
```

### Common Issues

**1. "No module named 'sentencepiece'"**
- **Cause:** Old requirements.txt cached
- **Fix:** Re-run deployment script or `pip install sentencepiece`

**2. "No module named 'ultralytics'"**
- **Cause:** Selfie worker missing dependency
- **Fix:** `pip install ultralytics>=8.1.0`

**3. Service won't start**
- Check logs for port conflicts
- Ensure old process killed: `ps aux | grep uvicorn`
- Kill manually: `kill -9 <PID>`

**4. CUDA out of memory**
- Selfie Feature needs 48GB VRAM (RTX 6000 Ada)
- Profile Worker needs 24GB VRAM (RTX 4090)

---

## ðŸ“Š Resource Requirements

| Service | VRAM | Startup Time | Model Size |
|---------|------|--------------|------------|
| Interface | 0 | <5s | - |
| Selfie Feature | ~20GB | 30-60s | Qwen2-VL-7B: 16GB |
| Profile Worker | ~5GB | 15-30s | Z-Image: 3GB |
| LLM Service | ~16GB | 20-40s | Qwen2.5-7B: 14GB |

---

## âœ… Verification Checklist

After deployment, verify:
- [ ] All 4 pods respond to health checks
- [ ] Interface can reach all workers
- [ ] Test selfie upload returns features
- [ ] Profile generation completes successfully
- [ ] No module import errors in logs
- [ ] GPU memory usage within limits

---

## ðŸ”„ Scaling Instructions

### Add More Selfie Feature Workers
1. Create new RTX 6000 Ada pod (48GB VRAM)
2. Deploy with `deploy_selfie_feature.sh`
3. Update `config/config.yaml`:
   ```yaml
   selfie_feature_worker_urls:
     - "https://wp6is8takig1z0-8003.proxy.runpod.net"
     - "https://NEW_POD_ID-8003.proxy.runpod.net"
   ```
4. Redeploy interface

### Add More Profile Workers
Same process, update `profile_worker_urls` in config

---

## ðŸ“ Files Modified

### Created
- âœ… `config/config.yaml` - Production configuration
- âœ… `deploy_llm.sh` - LLM service deployment script
- âœ… `FIXES_SUMMARY.md` - This document

### Updated
- âœ… `requirements.selfie_feature.txt` - Added sentencepiece, ultralytics
- âœ… `requirements.profile.txt` - Fixed base dependencies
- âœ… `requirements.llm.txt` - Added sentencepiece, updated versions
- âœ… `deploy_profile.sh` - Fixed requirements path

---

## ðŸŽ¯ Next Steps

1. **Push changes to GitHub:**
   ```bash
   git add -A
   git commit -m "Fix core dependencies and module errors across all pods"
   git push
   ```

2. **Deploy to all pods in order:**
   - Selfie Feature (longest download)
   - Profile Worker
   - LLM Service
   - Interface (last, connects to all)

3. **Run comprehensive tests**

4. **Monitor logs for 24h** to ensure stability

---

## ðŸ“ž Support

For issues:
1. Check logs first
2. Verify config.yaml URLs match current pods
3. Ensure git pull completed successfully
4. Restart service if needed

**All core module errors and dependency issues have been resolved.** âœ…
